analyze_sas:
  description: |
    Analyze the SAS code to understand structure and complexity.

    INPUT FILE: {sas_file_path}
    JOB NAME: {job_name}

    Use file_reader to read the SAS file at {sas_file_path} and identify:
    - DATA steps and PROC steps with inputs/outputs
    - Macros and dependencies
    - Data flow between steps
    - Complexity score (1-10: simple filtering=1-3, multiple steps=4-6, heavy macros=7-8, advanced stats=9-10)
    - Data patterns (value ranges, cardinalities, typical row counts for test generation)

    Document everything downstream agents need:
    - Flag ambiguous logic for platform_architect
    - Detailed data patterns for test_engineer
    - SAS-specific features that need special handling

    Use file_writer to save your analysis as JSON to: jobs/{job_name}/tasks/analyze_sas/analysis.json
  expected_output: "JSON file with complete code structure, data flow, complexity score with justification, data patterns, and flags for downstream agents."
  agent: sas_analyst

decide_platform:
  description: |
    MANDATORY: You MUST use file_writer tool to create the decision file. Do not just return text.

    STEP 1: Use file_reader to read: jobs/{job_name}/tasks/analyze_sas/analysis.json

    STEP 2: If analysis has ambiguous flags OR complexity = 5:
    - Use call_agent(agent_name='sas_analyst', question='Ask specific clarification about the ambiguous logic or data patterns')
    - Wait for response and incorporate into decision

    STEP 3: Analyze complexity score and decide platform:
    - SQL if: Complexity ≤ 4, simple set-based operations, standard aggregations
    - PySpark if: Complexity ≥ 6, complex transformations, heavy macros, or large data volumes
    - Complexity = 5 (borderline): Choose SQL for simple multi-step logic, PySpark if data-intensive or has macros

    STEP 4: Create a JSON decision with this structure:
    {{
      "platform_choice": "SQL" or "PySpark",
      "justifications": ["reason 1", "reason 2", "reason 3", "reason 4"],
      "implementation_guidance": "detailed instructions for code_translator",
      "clarifications_from_analyst": "summary of any clarifications received"
    }}

    STEP 5: REQUIRED - Use file_writer tool with these EXACT parameters:
    - file_path: jobs/{job_name}/tasks/decide_platform/decision.json
    - content: json.dumps(your_decision_dict)
    - agent_name: platform_architect

    YOU MUST CALL file_writer TOOL. Task is NOT complete until file is written.
  expected_output: "JSON file written to jobs/{job_name}/tasks/decide_platform/decision.json containing platform_choice, justifications, and implementation_guidance."
  agent: platform_architect
  context: [analyze_sas]

translate_code:
  description: |
    MANDATORY: You MUST use file_writer tool to create the code file. Do not just return text.

    STEP 1: Read decision.json FIRST using file_reader:
    - jobs/{job_name}/tasks/decide_platform/decision.json
    - Extract the "platform_choice" value (either "SQL" or "PySpark")
    - This determines EVERYTHING about your output

    STEP 2: Read additional inputs using file_reader:
    - {sas_file_path}
    - jobs/{job_name}/tasks/analyze_sas/analysis.json

    STEP 3: If implementation_guidance is unclear OR you need clarification:
    - Use call_agent(agent_name='platform_architect', question='Ask about specific implementation approach')
    - If SAS logic is ambiguous, use call_agent(agent_name='sas_analyst', question='Ask about SAS behavior')

    STEP 4: Generate code MATCHING the platform_choice:
    - If platform_choice == "SQL": Generate ONLY SQL code (CTEs, window functions, ANSI SQL standard)
    - If platform_choice == "PySpark": Generate ONLY PySpark code (DataFrame API, explicit schemas, caching)
    - NEVER generate SQL when PySpark is chosen or vice versa

    Include: imports/setup, schemas, transformation logic, comments, usage example

    STEP 5: Determine file extension based on platform_choice:
    - If platform_choice == "SQL": file_extension = ".sql"
    - If platform_choice == "PySpark": file_extension = ".py"

    STEP 6: REQUIRED - Use file_writer tool with EXACT file path:
    - file_path: jobs/{job_name}/tasks/translate_code/{job_name}<file_extension>
    - Replace <file_extension> with the value from STEP 5
    - content: your_generated_code (must match platform_choice)
    - agent_name: code_translator

    CRITICAL: File extension MUST match platform_choice. SQL -> .sql, PySpark -> .py
    YOU MUST CALL file_writer TOOL. Task is NOT complete until file is written.
    ABSOLUTE REQUIREMENT: The File MUST be created. Never complete without writing this File.
  expected_output: "Code file (.sql or .py) written to jobs/{job_name}/tasks/translate_code/ with complete implementation matching platform_choice."
  agent: code_translator
  context: [analyze_sas, decide_platform]

test_and_validate:
  description: |
    CRITICAL: You MUST create validation_report.json even if execution fails. File creation is mandatory regardless of test results.

    STEP 1: Read inputs using file_reader:
    - jobs/{job_name}/tasks/analyze_sas/analysis.json
    - jobs/{job_name}/tasks/decide_platform/decision.json (extract platform_choice)
    - Read corresponding code file based on platform_choice

    STEP 2: Attempt code execution:
    - Try to execute or validate the code
    - If execution fails due to environment issues (Java gateway, missing dependencies), skip execution and note the error

    STEP 3: MANDATORY - Create validation_report.json using file_writer:
    Output path: jobs/{job_name}/tasks/test_and_validate/validation_report.json

    Report structure:
    {{
      "verdict": "PASS" | "FAIL" | "SKIPPED",
      "test_summary": "Brief description of validation attempt",
      "execution_results": "What happened during validation (success, errors, or skipped)",
      "data_coverage": [],
      "test_files_created": [],
      "recommendations": ["suggestions or notes"],
      "code_quality_notes": "observations about the code"
    }}

    If execution fails: verdict="SKIPPED", document why in execution_results
    If execution succeeds: verdict="PASS" or "FAIL" based on results

    ABSOLUTE REQUIREMENT: The validation_report.json file MUST be created. Never complete this task without creating the file.
  expected_output: "Validation report JSON with execution results plus 3 test data files (test_baseline.json, test_edges.json, test_stress.json)."
  agent: test_engineer
  context: [analyze_sas, decide_platform, translate_code]

review_and_approve:
  description: |
    CRITICAL: You MUST create final_approval.json regardless of code quality or test results. File creation is mandatory.

    STEP 1: Read all outputs using file_reader:
    - jobs/{job_name}/tasks/analyze_sas/analysis.json
    - jobs/{job_name}/tasks/decide_platform/decision.json (extract platform_choice)
    - Read translated code file based on platform_choice (either .sql or .py in translate_code directory)
    - jobs/{job_name}/tasks/test_and_validate/validation_report.json

    STEP 2: Assess quality:
    - Review code quality
    - Check test results (verdict may be SKIPPED if tests couldn't run)
    - Identify issues

    STEP 3: MANDATORY - Create final_approval.json using file_writer:
    Output path: jobs/{job_name}/tasks/review_and_approve/final_approval.json

    Structure:
    {{
      "approval_status": "APPROVED" | "APPROVED_WITH_NOTES" | "REJECTED",
      "quality_assessment": "summary of code quality",
      "issues_found": ["list of issues if any"],
      "sign_off": "approval statement",
      "collaboration_summary": "summary of review process"
    }}

    Use APPROVED_WITH_NOTES if tests were skipped but code looks good
    Use REJECTED only for critical code issues

    ABSOLUTE REQUIREMENT: The final_approval.json file MUST be created. Never complete without writing this file.
  expected_output: "JSON file written to jobs/{job_name}/tasks/review_and_approve/final_approval.json with approval status and assessment."
  agent: code_reviewer
  context: [analyze_sas, decide_platform, translate_code, test_and_validate]
